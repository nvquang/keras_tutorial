{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How to apply word embedding using Gensim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The input machine learning model are matrices or vector but in natural language processing we only have documents so we need to convert a unit of document, word, to the vector and a document to a matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### One-hot vector - the naive method"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Simplest method we can apply first is one-hot vector, the idea of the method is very simple. Example, we have a vocabulary with size V, then we consider each word is a vector in V-dimension, only one target element being 1 and the others being 0. Let's look at this sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word:  I\n",
      "Vector:  [1. 0. 0. 0. 0.]\n",
      "Word:  love\n",
      "Vector:  [0. 1. 0. 0. 0.]\n",
      "Word:  cats\n",
      "Vector:  [0. 0. 1. 0. 0.]\n",
      "Word:  and\n",
      "Vector:  [0. 0. 0. 1. 0.]\n",
      "Word:  dogs\n",
      "Vector:  [0. 0. 0. 0. 1.]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "vocab = ['I', \"love\", \"cats\", \"and\", \"dogs\"] # assum the we only have small vocabulary\n",
    "vocab_size = len(vocab)\n",
    "\n",
    "for text_idx, word in enumerate(vocab):\n",
    "    one_hot = np.zeros(vocab_size)\n",
    "    one_hot[text_idx] = 1\n",
    "    print(\"Word: \", word)\n",
    "    print(\"Vector: \", one_hot)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Some issue with one-hot\n",
    "- First, you cannot infer any relationship between two words given their one-hot representation. In previous example, the word, the similarity between each words is equal and we cannot infer any information between words\n",
    "- Second, we are wasting a lot of space for 0 element. Look at previous example, there are only 5 words in vocabularty and we need spent a vector with 5-dimension."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word2Vec with Gensim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are two types of Word2Vec, Skip-gram and Continuous Bag of Words (CBOW). Let's me show both of method using gensim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Skip-gram\n",
    "For skip-gram, the input is the target word, while the outputs are the words surrounding the target words with pre-define window size. For each example, the sentence \"I love cats and dogs\" and windown size = 2, if the input is \"love\" then output is \"I\", \"cats\", \"and\". \n",
    "### Continuous Bag of Words (CBOW) \n",
    "It is  very similar to skip-gram, except that it swaps the input and output. The idea is that given a context, we want to know which word is most likely to appear in it. For example, given words \"I\", \"cats\" \"and\", the output is \"love\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I will show you how to perform word embedding with Gensim\n",
    "Install gensim:\n",
    " - Open terminal\n",
    " - type: pip install gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "from random import shuffle\n",
    "import re\n",
    "import urllib.request\n",
    "import zipfile\n",
    "import lxml.etree\n",
    "#download the data\n",
    "urllib.request.urlretrieve(\"https://wit3.fbk.eu/get.php?path=XML_releases/xml/ted_en-20160408.zip&filename=ted_en-20160408.zip\", filename=\"ted_en-20160408.zip\")\n",
    "# extract subtitle\n",
    "with zipfile.ZipFile('ted_en-20160408.zip', 'r') as z:\n",
    "    doc = lxml.etree.parse(z.open('ted_en-20160408.xml', 'r'))\n",
    "input_text = '\\n'.join(doc.xpath('//content/text()'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "24222849"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Size of input text\n",
    "len(input_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Here are two reasons companies fail: they only do more of the same, or they only do what's new.\\nTo m\""
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# First 100 charaters of input text\n",
    "input_text[:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Add some code to preprocessing data\n",
    "# remove parenthesis \n",
    "input_text_noparens = re.sub(r'\\([^)]*\\)', '', input_text)\n",
    "# store as list of sentences\n",
    "sentences_strings_ted = []\n",
    "for line in input_text_noparens.split('\\n'):\n",
    "    m = re.match(r'^(?:(?P<precolon>[^:]{,20}):)?(?P<postcolon>.*)$', line)\n",
    "    sentences_strings_ted.extend(sent for sent in m.groupdict()['postcolon'].split('.') if sent)\n",
    "# store as list of lists of words\n",
    "sentences_ted = []\n",
    "for sent_str in sentences_strings_ted:\n",
    "    tokens = re.sub(r\"[^a-z0-9]+\", \" \", sent_str.lower()).split()\n",
    "    sentences_ted.append(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see the sentences_ted array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "266694"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# len\n",
    "len(sentences_ted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['here',\n",
       "  'are',\n",
       "  'two',\n",
       "  'reasons',\n",
       "  'companies',\n",
       "  'fail',\n",
       "  'they',\n",
       "  'only',\n",
       "  'do',\n",
       "  'more',\n",
       "  'of',\n",
       "  'the',\n",
       "  'same',\n",
       "  'or',\n",
       "  'they',\n",
       "  'only',\n",
       "  'do',\n",
       "  'what',\n",
       "  's',\n",
       "  'new'],\n",
       " ['to',\n",
       "  'me',\n",
       "  'the',\n",
       "  'real',\n",
       "  'real',\n",
       "  'solution',\n",
       "  'to',\n",
       "  'quality',\n",
       "  'growth',\n",
       "  'is',\n",
       "  'figuring',\n",
       "  'out',\n",
       "  'the',\n",
       "  'balance',\n",
       "  'between',\n",
       "  'two',\n",
       "  'activities',\n",
       "  'exploration',\n",
       "  'and',\n",
       "  'exploitation']]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# first 10 elements\n",
    "sentences_ted[:2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the form that is ready to be fed into the Word2Vec model defined in Gensim. Word2Vec model can be easily trained with one line as the code below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "model_ted = Word2Vec(sentences=sentences_ted, size=100, window=5, min_count=5, workers=4, sg=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some important parameter:\n",
    "- sentences: list of sentences in all document\n",
    "- size: the dimensionality of the embedding vector\n",
    "- window: the number of context words you are looking at\n",
    "- min_count: tells the model to ignore words with total count less than this number.\n",
    "- workers: the number of threads being used\n",
    "- sg: whether to use skip-gram or CBOW"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's discovery some feature of Gensim model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('woman', 0.8459959030151367),\n",
       " ('guy', 0.8118977546691895),\n",
       " ('boy', 0.765205979347229),\n",
       " ('girl', 0.7609017491340637),\n",
       " ('lady', 0.7598025798797607),\n",
       " ('kid', 0.725189208984375),\n",
       " ('soldier', 0.7227253317832947),\n",
       " ('gentleman', 0.7090329527854919),\n",
       " ('poet', 0.6967312693595886),\n",
       " ('friend', 0.6757165193557739)]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# find the most similar of the \"man\"\n",
    "model_ted.wv.most_similar(\"man\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# the vector represent for the word \"man\"\n",
    "man_vector = model_ted.wv['man']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100,)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "man_vector.shape # It is equal the \"size\" parameter in the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 7.8068745e-01, -4.5165992e-01,  1.1014861e+00, -9.3772238e-01,\n",
       "        1.6919795e-01, -1.9153500e+00,  2.4239585e-01,  1.5283470e-02,\n",
       "        1.9348862e+00, -1.2310663e+00, -1.8260899e+00,  2.2298059e-01,\n",
       "       -7.5558901e-01, -1.9010946e+00, -1.8604414e+00, -4.2120236e-01,\n",
       "       -6.2933797e-01, -8.3188128e-01, -2.2161929e-02,  4.7784784e-01,\n",
       "       -2.0343399e+00, -1.5837117e+00,  1.8361408e-02, -9.8428136e-01,\n",
       "       -1.0060577e+00,  4.3728873e-01,  9.1070540e-02, -1.5524529e+00,\n",
       "        4.5749509e-01,  1.7482825e-01,  2.0538163e+00, -1.9110399e+00,\n",
       "        2.0106401e+00, -1.6175275e-01, -2.1399779e+00,  1.4197608e+00,\n",
       "        9.6490607e-03, -2.4535669e-01, -1.3951262e+00, -5.0295997e-01,\n",
       "        7.5743324e-01, -7.4039704e-01, -1.1575367e+00, -7.0345587e-01,\n",
       "       -9.2523569e-01, -1.9069070e-01, -3.4693712e-03,  2.6064306e-01,\n",
       "        9.4525474e-01,  1.8784159e+00, -8.4275073e-01,  1.2200514e+00,\n",
       "        1.1355350e+00,  1.4757791e+00, -2.6515985e+00,  1.3076806e-01,\n",
       "        5.1560944e-01,  1.4360324e-01,  1.0551065e-01, -3.9415938e-01,\n",
       "       -6.0459733e-01,  4.2071611e-01,  1.1142797e+00, -7.8444421e-01,\n",
       "       -7.3440093e-01, -1.0979464e+00, -4.6059325e-02, -1.4970373e-01,\n",
       "        2.5691265e-01, -7.5219005e-01, -4.6016015e-02,  2.9421860e-01,\n",
       "        9.9991554e-01, -7.2643185e-01, -2.4719743e-01, -5.5278373e-01,\n",
       "        1.5751472e+00, -9.9863923e-01, -3.5655302e-01,  1.0985218e+00,\n",
       "       -7.0417905e-01,  4.5528692e-01,  8.0895442e-01,  8.7937510e-01,\n",
       "        1.8890722e+00,  1.7003913e+00, -3.9904746e-01,  1.5472724e+00,\n",
       "        1.5813684e+00, -3.6034324e+00,  6.3402456e-01,  1.2178830e+00,\n",
       "        2.6550147e-01, -1.6928146e+00,  4.6567863e-01, -7.9538572e-01,\n",
       "        1.3239202e+00,  7.8379643e-01,  1.1213299e+00, -1.0003985e+00],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "man_vector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The biggest issue of Word2Vec is that it is not able to represent words that do not appear in the training dataset. Example let's see what the vector represent for the word \"I\", we will get the error \"word 'I' not in vocabulary\" because we remove this word in preprocess steps. We will overcome this issue by training with larger vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "\"word 'I' not in vocabulary\"",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-38-5a54f4e46575>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel_ted\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwv\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'I'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/gensim/models/keyedvectors.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, words)\u001b[0m\n\u001b[1;32m    324\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwords\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstring_types\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    325\u001b[0m             \u001b[0;31m# allow calls like trained_model['office'], as a shorthand for trained_model[['office']]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 326\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mword_vec\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwords\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    327\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    328\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mvstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mword_vec\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mword\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mwords\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/gensim/models/keyedvectors.py\u001b[0m in \u001b[0;36mword_vec\u001b[0;34m(self, word, use_norm)\u001b[0m\n\u001b[1;32m    451\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    452\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 453\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"word '%s' not in vocabulary\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mword\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    454\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    455\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mmost_similar\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpositive\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnegative\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtopn\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrestrict_vocab\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindexer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: \"word 'I' not in vocabulary\""
     ]
    }
   ],
   "source": [
    "model_ted.wv['I']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Referenece link\n",
    " - https://radimrehurek.com/gensim/\n",
    " - https://radimrehurek.com/gensim/install.html\n",
    " - https://towardsdatascience.com/word-embedding-with-word2vec-and-fasttext-a209c1d3e12c\n",
    " - https://radimrehurek.com/gensim/tut1.html\n",
    " - https://www.depends-on-the-definition.com/guide-to-word-vectors-with-gensim-and-keras/"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
